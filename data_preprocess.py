import os
import pandas as pd
import numpy as np
from PIL import Image
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from torchvision import transforms
import torch
import warnings
warnings.filterwarnings('ignore')

class MVSAImageTextDataset(Dataset):
    """MVSAæ ¼å¼æ•°æ®é›†åŠ è½½å™¨ - æ”¯æŒè®­ç»ƒ/éªŒè¯/æµ‹è¯•ä¸‰æ¨¡å¼"""
    
    def __init__(
        self,
        data_dir: str,
        label_file: str,
        mode: str = 'train',  # 'train', 'val', 'test'
        img_size: int = 224,
        max_text_len: int = 128,
        balance_classes: bool = False  # æ˜¯å¦å¯ç”¨ç±»åˆ«å¹³è¡¡
    ):
        """
        Args:
            data_dir: å­˜æ”¾.jpg/.txtæ–‡ä»¶çš„ç›®å½• (e.g., '/project5/data')
            label_file: æ ‡ç­¾CSVæ–‡ä»¶è·¯å¾„ (train.txt æˆ– test_without_label.txt)
            mode: 'train'/'val'/'test' - æµ‹è¯•é›†è‡ªåŠ¨å¿½ç•¥æ ‡ç­¾
            img_size: å›¾åƒç¼©æ”¾å°ºå¯¸
            max_text_len: æ–‡æœ¬æœ€å¤§é•¿åº¦
            balance_classes: è®­ç»ƒæ—¶æ˜¯å¦å¯ç”¨ç±»åˆ«å¹³è¡¡é‡‡æ ·
        """
        self.data_dir = data_dir
        self.mode = mode
        self.max_text_len = max_text_len
        
        # å›¾åƒé¢„å¤„ç† (ImageNetæ ‡å‡†åŒ–)
        self.transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
        ])
        
        # åŠ è½½æ ‡ç­¾æ–‡ä»¶
        self.df = pd.read_csv(label_file)
        self.df = self.df[self.df['guid'].notna()]  # ç§»é™¤ç©ºguid
        
        # æ•°æ®æ¸…æ´—ä¸éªŒè¯
        self._validate_and_clean()
        
        # ç±»åˆ«æ˜ å°„ (ä¸CDANè®ºæ–‡ä¸€è‡´)
        self.label_map = {'positive': 0, 'neutral': 1, 'negative': 2}
        self.reverse_label_map = {v: k for k, v in self.label_map.items()}
        
        # ä»…è®­ç»ƒ/éªŒè¯æ¨¡å¼éœ€è¦æ ‡ç­¾
        if mode != 'test':
            # è¿‡æ»¤æ— æ•ˆæ ‡ç­¾
            valid_mask = self.df['tag'].isin(self.label_map.keys())
            invalid_count = (~valid_mask).sum()
            if invalid_count > 0:
                print(f"âš ï¸ è­¦å‘Š: è¿‡æ»¤ {invalid_count} ä¸ªæ— æ•ˆæ ‡ç­¾ (épositive/neutral/negative)")
                self.df = self.df[valid_mask].reset_index(drop=True)
            
            # ç”Ÿæˆæ ‡ç­¾ç´¢å¼•
            self.labels = self.df['tag'].map(self.label_map).values.astype(np.int64)
            
            # ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡
            self._print_class_distribution()
            
            # ç±»åˆ«å¹³è¡¡æƒé‡ (ç”¨äºWeightedRandomSampler)
            if balance_classes and mode == 'train':
                self.class_weights = self._compute_class_weights()
            else:
                self.class_weights = None
        else:
            self.labels = None
            self.class_weights = None
    
    def _validate_and_clean(self):
        """éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§å¹¶æ¸…ç†æ— æ•ˆæ ·æœ¬"""
        valid_indices = []
        missing_files = []
        
        for idx, row in self.df.iterrows():
            guid = int(row['guid'])
            img_path = os.path.join(self.data_dir, f"{guid}.jpg")
            txt_path = os.path.join(self.data_dir, f"{guid}.txt")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not (os.path.exists(img_path) and os.path.exists(txt_path)):
                missing_files.append(guid)
                continue
            
            # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºç©º
            try:
                with open(txt_path, 'r', encoding='utf-8', errors='ignore') as f:
                    text = f.read().strip()
                if not text:
                    continue
            except Exception:
                continue
            
            valid_indices.append(idx)
        
        # åº”ç”¨è¿‡æ»¤
        original_len = len(self.df)
        self.df = self.df.iloc[valid_indices].reset_index(drop=True)
        
        # æ‰“å°æ¸…ç†æŠ¥å‘Š
        if missing_files:
            print(f"â–¶ æ¸…ç†æŠ¥å‘Š: åŸå§‹æ ·æœ¬æ•°={original_len}, æœ‰æ•ˆæ ·æœ¬æ•°={len(self.df)}")
            print(f"   - ç¼ºå¤±æ–‡ä»¶æ ·æœ¬æ•°: {len(missing_files)} (ç¤ºä¾‹: {missing_files[:5]})")
        else:
            print(f"âœ” æ•°æ®éªŒè¯é€šè¿‡: å…± {len(self.df)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
    
    def _print_class_distribution(self):
        """æ‰“å°ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡"""
        if self.labels is None:
            return
        
        total = len(self.labels)
        counts = np.bincount(self.labels, minlength=3)
        ratios = counts / total * 100
        
        print("\nâ–¶ ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:")
        for i, label_name in enumerate(['positive', 'neutral', 'negative']):
            print(f"   {label_name:12s}: {counts[i]:5d} æ ·æœ¬ ({ratios[i]:5.2f}%)")
        print(f"   {'æ€»è®¡':12s}: {total:5d} æ ·æœ¬")
        
        # æ£€æµ‹ä¸¥é‡ä¸å¹³è¡¡ (negative < 15%)
        if ratios[2] < 15.0:
            print(f"Ã—  è­¦å‘Š: negativeç±»åˆ«å æ¯”è¿‡ä½({ratios[2]:.2f}%)ï¼Œå»ºè®®å¯ç”¨balance_classes=True")
    
    def _compute_class_weights(self):
        """è®¡ç®—ç±»åˆ«æƒé‡ç”¨äºå¹³è¡¡é‡‡æ · (è§£å†³MVSAæ•°æ®é›†ä¸å¹³è¡¡é—®é¢˜)"""
        counts = np.bincount(self.labels, minlength=3)
        weights = 1.0 / (counts + 1e-5)  # é¿å…é™¤é›¶
        weights = weights / weights.sum() * len(weights)  # å½’ä¸€åŒ–
        sample_weights = weights[self.labels]
        return torch.from_numpy(sample_weights).float()
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        guid = int(self.df.iloc[idx]['guid'])
        img_path = os.path.join(self.data_dir, f"{guid}.jpg")
        txt_path = os.path.join(self.data_dir, f"{guid}.txt")
        
        # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
        try:
            image = Image.open(img_path).convert('RGB')
            image = self.transform(image)
        except Exception as e:
            # å›¾åƒåŠ è½½å¤±è´¥æ—¶è¿”å›é›¶å¼ é‡ (è®­ç»ƒä¸­ç½•è§ï¼Œä½†éœ€é²æ£’å¤„ç†)
            print(f"Ã— å›¾åƒåŠ è½½å¤±è´¥ {img_path}: {e}")
            image = torch.zeros(3, 224, 224)
        
        # åŠ è½½å¹¶æ¸…ç†æ–‡æœ¬
        try:
            with open(txt_path, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read().strip()
            
            # æ–‡æœ¬æ¸…æ´—: ç§»é™¤URL/ç‰¹æ®Šå­—ç¬¦ (MVSAå¸¸è§å™ªå£°)
            import re
            text = re.sub(r'http\S+|www\S+', '', text)  # ç§»é™¤URL
            text = re.sub(r'@\w+|#\w+', '', text)       # ç§»é™¤@æåŠå’Œ#æ ‡ç­¾
            text = re.sub(r'\s+', ' ', text).strip()     # åˆå¹¶å¤šä½™ç©ºæ ¼
            
            # 2. é•¿åº¦é¢„æˆªæ–­ï¼ˆé¢„é˜²CLIPæˆªæ–­ä¸¢å¤±å…³é”®æƒ…æ„Ÿè¯ï¼‰
            if len(text) > 200:  # 200å­—ç¬¦ â‰ˆ 40-50 tokens
                text = text[:200] + "..."  # ä¿ç•™å¼€å¤´å…³é”®ä¿¡æ¯
            
            if not text:
                text = "[EMPTY]"  # ç©ºæ–‡æœ¬å ä½ç¬¦
        except Exception as e:
            print(f"Ã— æ–‡æœ¬åŠ è½½å¤±è´¥ {txt_path}: {e}")
            text = "[ERROR]"
        
        # è¿”å›æ•°æ®ç»“æ„
        sample = {
            'guid': guid,
            'image': image,
            'text': text[:self.max_text_len]  # æˆªæ–­é•¿æ–‡æœ¬
        }
        
        # ä»…è®­ç»ƒ/éªŒè¯æ¨¡å¼è¿”å›æ ‡ç­¾
        if self.labels is not None:
            sample['label'] = torch.tensor(self.labels[idx], dtype=torch.long)
        
        return sample


def create_dataloaders(
    data_dir: str,
    train_label_file: str,
    test_label_file: str,
    batch_size: int = 16,
    val_split: float = 0.15,  # éªŒè¯é›†æ¯”ä¾‹
    balance_classes: bool = True,  # å¯ç”¨ç±»åˆ«å¹³è¡¡é‡‡æ ·
    num_workers: int = 4,
    seed: int = 42
):
    """
    åˆ›å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•DataLoader (è‡ªåŠ¨åˆ’åˆ†éªŒè¯é›†)
    
    Returns:
        train_loader, val_loader, test_loader, class_weights (ç”¨äºæŸå¤±å‡½æ•°)
    """
    # å›ºå®šéšæœºç§å­ä¿è¯å¯å¤ç°æ€§
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    # 1. åˆ›å»ºå®Œæ•´è®­ç»ƒé›† (å«æ ‡ç­¾)
    full_train_dataset = MVSAImageTextDataset(
        data_dir=data_dir,
        label_file=train_label_file,
        mode='train',
        balance_classes=balance_classes
    )
    
    # 2. æŒ‰ç±»åˆ«åˆ†å±‚åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† (è§£å†³ä¸å¹³è¡¡æ•°æ®åˆ’åˆ†åå·®)
    indices = np.arange(len(full_train_dataset))
    labels = full_train_dataset.labels
    
    train_indices, val_indices = [], []
    for class_id in np.unique(labels):
        class_indices = indices[labels == class_id]
        np.random.shuffle(class_indices)
        split_point = int(len(class_indices) * (1 - val_split))
        train_indices.extend(class_indices[:split_point])
        val_indices.extend(class_indices[split_point:])
    
    # åˆ›å»ºå­é›†
    train_dataset = torch.utils.data.Subset(full_train_dataset, train_indices)
    val_dataset = torch.utils.data.Subset(full_train_dataset, val_indices)
    
    # 3. åˆ›å»ºæµ‹è¯•é›† (æ— æ ‡ç­¾)
    test_dataset = MVSAImageTextDataset(
        data_dir=data_dir,
        label_file=test_label_file,
        mode='test'
    )
    
    # 4. æ„å»ºDataLoader
    # è®­ç»ƒé›†: å¯ç”¨åŠ æƒé‡‡æ ·è§£å†³ç±»åˆ«ä¸å¹³è¡¡
    if balance_classes and full_train_dataset.class_weights is not None:
        train_weights = full_train_dataset.class_weights[train_indices]
        sampler = WeightedRandomSampler(
            weights=train_weights,
            num_samples=len(train_weights),
            replacement=True
        )
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            sampler=sampler,
            num_workers=num_workers,
            pin_memory=True
        )
    else:
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True
        )
    
    # éªŒè¯/æµ‹è¯•é›†: é¡ºåºé‡‡æ ·
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size * 2,  # éªŒè¯å¯æ›´å¤§batch
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size * 2,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    # 5. è¿”å›ç±»åˆ«æƒé‡ (ç”¨äºæŸå¤±å‡½æ•°åŠ æƒ)
    # æ ¹æ®MVSAåˆ†å¸ƒ: positive 59.5%, neutral 30.1%, negative 10.4%
    class_weights = torch.tensor([1.0, 1.8, 5.0], dtype=torch.float)  # negativeæƒé‡æœ€é«˜
    
    print(f"\nâœ” DataLoaderæ„å»ºå®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬ | éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬ | æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    print(f"   Batch Size: {batch_size} | å¯ç”¨ç±»åˆ«å¹³è¡¡: {balance_classes}")
    
    return train_loader, val_loader, test_loader, class_weights

def create_fixed_val_loader(data_dir, val_guid_file, batch_size=32, num_workers=4):
    """
    åˆ›å»ºå›ºå®šéªŒè¯é›†DataLoaderï¼ˆä¸é‡æ–°åˆ’åˆ†ï¼‰
    """
    # è¯»å–ä¿å­˜çš„GUID
    val_guids = pd.read_csv(val_guid_file)['guid'].tolist()
    
    # åˆ›å»ºä»…åŒ…å«è¿™äº›GUIDçš„Dataset
    class FixedValDataset(MVSAImageTextDataset):
        def __init__(self, data_dir, guid_list):
            self.data_dir = data_dir
            self.guid_list = guid_list
            
            # ä¸´æ—¶åˆ›å»ºå®Œæ•´dfç”¨äºè¿‡æ»¤
            temp_df = pd.read_csv(r"E:\data\project5\project5\train.txt")  # æ›¿æ¢ä¸ºä½ çš„æ ‡ç­¾æ–‡ä»¶è·¯å¾„
            self.df = temp_df[temp_df['guid'].isin(guid_list)].reset_index(drop=True)
            
            # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼ˆè·³è¿‡é‡å¤éªŒè¯ï¼‰
            self.mode = 'val'
            self.max_text_len = 128
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor()
            ])
            self.label_map = {'positive': 0, 'neutral': 1, 'negative': 2}
            
            # ç”Ÿæˆæ ‡ç­¾
            valid_mask = self.df['tag'].isin(self.label_map.keys())
            self.df = self.df[valid_mask].reset_index(drop=True)
            self.labels = self.df['tag'].map(self.label_map).values.astype(np.int64)
    
    dataset = FixedValDataset(data_dir, val_guids)
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    return loader, dataset.labels

# ==================== main ====================
if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    DATA_DIR = r"E:\data\project5\project5\data"
    TRAIN_LABEL = r"E:\data\project5\project5\train.txt"
    TEST_LABEL = r"E:\data\project5\project5\test_without_label.txt"
    
    # åˆ›å»ºDataLoader
    train_loader, val_loader, test_loader, class_weights = create_dataloaders(
        data_dir=DATA_DIR,
        train_label_file=TRAIN_LABEL,
        test_label_file=TEST_LABEL,
        batch_size=16,
        val_split=0.15,
        balance_classes=True,  # è§£å†³negativeæ ·æœ¬ç¨€å°‘é—®é¢˜
        num_workers=4
    )
    
    # éªŒè¯æ•°æ®åŠ è½½
    print("\nâ–¶ éªŒè¯æ•°æ®åŠ è½½ (è®­ç»ƒé›†é¦–batch):")
    for batch in train_loader:
        print(f"   å›¾åƒå½¢çŠ¶: {batch['image'].shape}")
        print(f"   æ–‡æœ¬ç¤ºä¾‹: {batch['text'][:3]}")
        print(f"   æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(batch['label'].numpy(), minlength=3)}")
        print(f"   GUIDç¤ºä¾‹: {batch['guid'][:3].tolist()}")
        break
    
    # éªŒè¯æµ‹è¯•é›†åŠ è½½
    print("\nâ–¶ éªŒè¯æµ‹è¯•é›†åŠ è½½:")
    for batch in test_loader:
        print(f"   æµ‹è¯•å›¾åƒå½¢çŠ¶: {batch['image'].shape}")
        print(f"   æµ‹è¯•æ–‡æœ¬ç¤ºä¾‹: {batch['text'][:3]}")
        print(f"   æµ‹è¯•GUID: {batch['guid'][:3].tolist()}")
        break
    
    print(f"\nğŸ’¡ æç¤º: class_weights = {class_weights.tolist()} å¯ç”¨äºæŸå¤±å‡½æ•°:")
    print("   criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))")