import os
import sys
import time
import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from tqdm import tqdm_notebook as tqdm
import warnings
warnings.filterwarnings('ignore')

# ======================= ã€é…ç½®ã€‘ ======================
CONFIG = {
    # --- è·¯å¾„é…ç½® ---
    'data_dir': r"E:\data\project5\project5\data",           
    'train_label_file': r"E:\data\project5\project5\train.txt",
    'test_label_file': r"E:\data\project5\project5\test_without_label.txt",
    'output_dir': r"E:\data\project5\project5\output",       # è¾“å‡ºç›®å½•
    
    # --- æœ¬åœ°æ¨¡å‹è·¯å¾„ (å…³é”®!) ---
    'clip_path': r"E:\data\project5\project5\clip",      # CLIPæ¨¡å‹
    'bert_path': r"E:\data\project5\project5\bert",          # BERTæ¨¡å‹
    'vit_path': r"E:\data\project5\project5\vit",        # ViTæ¨¡å‹
    
    # --- è®­ç»ƒé…ç½® ---
    'mode': 'train_test',                    # 'train', 'test', 'train_test'
    'epochs': 30,
    'batch_size': 16,
    'learning_rate': 2e-5,
    'weight_decay': 0.01,
    'val_split': 0.15,                       # éªŒè¯é›†æ¯”ä¾‹
    'balance_classes': True,                 
    'patience': 10,                          
    
    # --- ç³»ç»Ÿé…ç½® ---
    'cpu': False,                            # True=å¼ºåˆ¶CPU, False=è‡ªåŠ¨æ£€æµ‹GPU
    'num_workers': 4,
    'seed': 1,
    
    # --- è°ƒè¯•é…ç½® ---
    'debug_samples': 0,                      
}
# ===========================================================

# é¡¹ç›®æ¨¡å—å¯¼å…¥
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from cdan_model import CDAN
from data_preprocess import create_dataloaders, MVSAImageTextDataset

# å›ºå®šéšæœºç§å­
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class Trainer:
    """CDANè®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() and not config['cpu'] else "cpu")
        print(f"â–¶ [è®¾å¤‡] ä½¿ç”¨ {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(config['output_dir'], exist_ok=True)
        self.writer = SummaryWriter(log_dir=os.path.join(config['output_dir'], 'tensorboard'))
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        print("\nâ–¶ [æ•°æ®] åŠ è½½æ•°æ®é›†...")
        self.train_loader, self.val_loader, self.test_loader, self.class_weights = create_dataloaders(
            data_dir=config['data_dir'],
            train_label_file=config['train_label_file'],
            test_label_file=config['test_label_file'],
            batch_size=config['batch_size'],
            val_split=config['val_split'],
            balance_classes=config['balance_classes'],
            num_workers=config['num_workers'],
            seed=config['seed']
        )
        
        # è°ƒè¯•æ¨¡å¼: é™åˆ¶æ ·æœ¬æ•°
        if config['debug_samples'] > 0:
            print(f"ğŸ [è°ƒè¯•] ä»…ä½¿ç”¨ {config['debug_samples']} ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€ŸéªŒè¯")
            # æ›¿æ¢DataLoaderä¸ºå°æ‰¹é‡ç‰ˆæœ¬
            def create_debug_loader(loader, n_samples):
                data = []
                for i, batch in enumerate(loader):
                    if i * loader.batch_size >= n_samples:
                        break
                    data.append(batch)
                return data
            self.train_loader = create_debug_loader(self.train_loader, config['debug_samples'])
            self.val_loader = create_debug_loader(self.val_loader, max(10, config['debug_samples']//5))
            self.test_loader = create_debug_loader(self.test_loader, max(10, config['debug_samples']//5))
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("\nâ–¶ [æ¨¡å‹] åˆå§‹åŒ–CDAN...")
        self.model = CDAN(
            num_classes=3,
            feat_dim=512,
            cmsa_layers=8,
            cmsa_heads=8,
            config=config
        ).to(self.device)
        
        # å¤šGPUæ”¯æŒ
        if torch.cuda.device_count() > 1 and not config['cpu']:
            print(f"ParallelGroup: æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPUï¼Œå¯ç”¨DataParallel")
            self.model = nn.DataParallel(self.model)
        
        # æŸå¤±å‡½æ•° (é’ˆå¯¹MVSAä¸å¹³è¡¡åˆ†å¸ƒ)
        self.criterion_ce = nn.CrossEntropyLoss(
            weight=self.class_weights.to(self.device) if self.class_weights is not None else None
        )
        self.criterion_recon = nn.MSELoss()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay'],
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config['epochs'],
            eta_min=1e-6
        )
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_acc = 0.0
        self.best_val_f1 = 0.0
        self.patience_counter = 0
        self.global_step = 0
        
        # æ ‡ç­¾æ˜ å°„
        self.label_map = {0: 'positive', 1: 'neutral', 2: 'negative'}
    
    def train_epoch(self, epoch):
        """å•è½®è®­ç»ƒ"""
        self.model.train()
        total_loss, total_ce_loss, total_recon_loss = 0.0, 0.0, 0.0
        correct, total = 0, 0
        
        # ä½¿ç”¨tqdm_notebookå…¼å®¹Spyder/Jupyter
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{self.config['epochs']} [Train]", 
                    leave=True, file=sys.stdout)
        
        for batch_idx, batch in enumerate(pbar):
            # å¤„ç†è°ƒè¯•æ¨¡å¼çš„batchæ ¼å¼
            if isinstance(batch, list):
                if batch_idx >= len(self.train_loader):
                    break
                batch = self.train_loader[batch_idx]
            
            images = batch['image'].to(self.device)
            texts = batch['text']
            labels = batch['label'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(images, texts, labels)
            loss = outputs['loss']
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            total_ce_loss += outputs['ce_loss'].item()
            total_recon_loss += outputs['recon_loss'].item()
            
            preds = torch.argmax(outputs['logits'], dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f"{loss.item():.4f}",
                'Acc': f"{100.*correct/total:.1f}%"
            })
            
            # TensorBoardè®°å½•
            if self.global_step % 10 == 0:
                self.writer.add_scalar('train/loss', loss.item(), self.global_step)
                self.writer.add_scalar('train/accuracy', 100.*correct/total, self.global_step)
            
            self.global_step += 1
        
        pbar.close()
        
        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100. * correct / total
        
        print(f"\nâ–¶ è®­ç»ƒè½®æ¬¡ {epoch+1} å®Œæˆ | Loss: {avg_loss:.4f} | Acc: {accuracy:.2f}%")
        self.writer.add_scalar('epoch/train_loss', avg_loss, epoch)
        self.writer.add_scalar('epoch/train_acc', accuracy, epoch)
        
        return avg_loss, accuracy
    
    @torch.no_grad()
    def validate(self, epoch):
        """éªŒè¯é›†è¯„ä¼°"""
        self.model.eval()
        total_loss = 0.0
        all_preds, all_labels = [], []
        
        pbar = tqdm(self.val_loader, desc=f"Epoch {epoch+1}/{self.config['epochs']} [Val]", 
                    leave=True, file=sys.stdout)
        
        for batch in pbar:
            if isinstance(batch, list):
                if not self.val_loader:
                    break
                batch = self.val_loader[0] if not batch else batch
            
            images = batch['image'].to(self.device)
            texts = batch['text']
            labels = batch['label'].to(self.device)
            
            outputs = self.model(images, texts, labels)
            loss = outputs['loss']
            total_loss += loss.item()
            
            preds = torch.argmax(outputs['logits'], dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            pbar.set_postfix({'Loss': f"{loss.item():.4f}"})
        
        pbar.close()
        
        avg_loss = total_loss / (len(self.val_loader) if not isinstance(self.val_loader, list) else max(1, len(self.val_loader)))
        accuracy = 100. * sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)
        weighted_f1 = 100. * f1_score(all_labels, all_preds, average='weighted')
        
        # æ¯3è½®æ‰“å°è¯¦ç»†æŠ¥å‘Š
        if (epoch + 1) % 3 == 0:
            print("\nâ–¶ åˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(all_labels, all_preds, 
                                      target_names=['positive', 'neutral', 'negative']))
        
        print(f"â–¶ éªŒè¯è½®æ¬¡ {epoch+1} | Loss: {avg_loss:.4f} | Acc: {accuracy:.2f}% | F1: {weighted_f1:.2f}%")
        
        self.writer.add_scalar('val/loss', avg_loss, epoch)
        self.writer.add_scalar('val/accuracy', accuracy, epoch)
        self.writer.add_scalar('val/f1', weighted_f1, epoch)
        
        return avg_loss, accuracy, weighted_f1
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "="*70)
        print("â–¶ å¯åŠ¨CDANè®­ç»ƒ (Spyder IDEæ¨¡å¼)")
        print("="*70)
        print(f"  æ•°æ®ç›®å½•: {self.config['data_dir']}")
        print(f"  è¾“å‡ºç›®å½•: {self.config['output_dir']}")
        print(f"  Batch Size: {self.config['batch_size']} | Epochs: {self.config['epochs']}")
        print(f"  ç±»åˆ«å¹³è¡¡: {'å¯ç”¨' if self.config['balance_classes'] else 'ç¦ç”¨'}")
        if self.config['debug_samples'] > 0:
            print(f"  â–¶  è°ƒè¯•æ¨¡å¼: ä»…ä½¿ç”¨ {self.config['debug_samples']} ä¸ªæ ·æœ¬")
        print("="*70 + "\n")
        
        training_start = time.time()
        
        for epoch in range(self.config['epochs']):
            epoch_start = time.time()
            
            # è®­ç»ƒ + éªŒè¯
            self.train_epoch(epoch)
            val_loss, val_acc, val_f1 = self.validate(epoch)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.best_val_f1 = val_f1
                self.patience_counter = 0
                
                model_path = os.path.join(self.config['output_dir'], 'best_model.pth')
                save_dict = {
                    'epoch': epoch,
                    'model_state_dict': self.model.module.state_dict() if isinstance(self.model, nn.DataParallel) else self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'best_val_acc': self.best_val_acc,
                    'best_val_f1': self.best_val_f1,
                    'config': self.config
                }
                torch.save(save_dict, model_path)
                print(f"â–¶ ä¿å­˜æœ€ä½³æ¨¡å‹ (Acc: {val_acc:.2f}%, F1: {val_f1:.2f}%) â†’ {model_path}")
            else:
                self.patience_counter += 1
                print(f"â–¶ æ—©åœè®¡æ•°: {self.patience_counter}/{self.config['patience']}")
            
            # æ—©åœ
            if self.config['patience'] > 0 and self.patience_counter >= self.config['patience']:
                print(f"\nâ–¶ è§¦å‘æ—©åœ (æ— æ”¹è¿› {self.patience_counter} è½®)")
                break
            
            epoch_time = time.time() - epoch_start
            print(f"â–¶  è½®æ¬¡è€—æ—¶: {epoch_time:.2f}ç§’ | å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}\n")
        
        training_time = time.time() - training_start
        print("\n" + "="*70)
        print(f"â–¶ è®­ç»ƒå®Œæˆ | æ€»è€—æ—¶: {training_time/60:.2f} åˆ†é’Ÿ")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.2f}%")
        print(f"   æœ€ä½³éªŒè¯F1: {self.best_val_f1:.2f}%")
        print("="*70)
        self.writer.close()
    
    @torch.no_grad()
    def test(self):
        """æµ‹è¯•é›†é¢„æµ‹"""
        print("\nâ–¶ å¯åŠ¨æµ‹è¯•é¢„æµ‹...")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        model_path = os.path.join(self.config['output_dir'], 'best_model.pth')
        if os.path.exists(model_path):
            print(f"â–¶ åŠ è½½æ¨¡å‹: {model_path}")
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            state_dict = checkpoint['model_state_dict']
            
            # å¤„ç†DataParallel
            if isinstance(self.model, nn.DataParallel):
                self.model.module.load_state_dict(state_dict)
            else:
                if list(state_dict.keys())[0].startswith('module.'):
                    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
                self.model.load_state_dict(state_dict)
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼Œä½¿ç”¨å½“å‰æƒé‡")
        
        self.model.eval()
        results = []
        
        pbar = tqdm(self.test_loader, desc="æµ‹è¯•é¢„æµ‹", leave=True, file=sys.stdout)
        for batch in pbar:
            if isinstance(batch, list):
                if not self.test_loader:
                    break
                batch = self.test_loader[0] if not batch else batch
            
            images = batch['image'].to(self.device)
            texts = batch['text']
            guids = batch['guid'].numpy()
            
            outputs = self.model(images, texts)
            preds = torch.argmax(outputs['logits'], dim=1).cpu().numpy()
            
            for guid, pred in zip(guids, preds):
                results.append({'guid': int(guid), 'tag': self.label_map[pred]})
        
        pbar.close()
        
        # ä¿å­˜ç»“æœ
        results_df = pd.DataFrame(results).sort_values('guid').reset_index(drop=True)
        submission_path = os.path.join(self.config['output_dir'], 'submission.csv')
        results_df.to_csv(submission_path, index=False)
        
        print(f"\nâ–¶ é¢„æµ‹å®Œæˆ | æ ·æœ¬æ•°: {len(results_df)}")
        print(f"   ç»“æœä¿å­˜è‡³: {submission_path}")
        print(f"   æ ‡ç­¾åˆ†å¸ƒ: {results_df['tag'].value_counts().to_dict()}")
        
        # æ˜¾ç¤ºå‰10æ¡é¢„æµ‹
        print("\nâ–¶ å‰10æ¡é¢„æµ‹ç¤ºä¾‹:")
        print(results_df.head(10).to_string(index=False))
        
        return results_df
    
def save_validation_guids(config):
    """ä»…åˆ›å»ºdataloaderï¼Œä¸è®­ç»ƒï¼Œç›´æ¥ä¿å­˜éªŒè¯é›†GUID"""
    print("\n" + "="*60)
    print("â–¶ ä¿å­˜éªŒè¯é›†GUIDï¼ˆæ— éœ€è®­ç»ƒï¼‰")
    print("="*60)
    
    # å›ºå®šç§å­ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰
    set_seed(config['seed'])
    
    # åˆ›å»ºdataloaderï¼ˆéªŒè¯é›†åœ¨æ­¤åˆ»ç¡®å®šï¼‰
    _, val_loader, _, _ = create_dataloaders(
        data_dir=config['data_dir'],
        train_label_file=config['train_label_file'],
        test_label_file=config['test_label_file'],
        batch_size=config['batch_size'],
        val_split=config['val_split'],
        balance_classes=False,  # ä¿å­˜åŸå§‹åˆ†å¸ƒï¼Œä¸éœ€è¿‡é‡‡æ ·
        num_workers=config['num_workers'],
        seed=config['seed']
    )
    
    # æ”¶é›†æ‰€æœ‰éªŒè¯é›†GUID
    val_guids = []
    for batch in val_loader:
        val_guids.extend(batch['guid'].numpy().tolist())
    
    # ä¿å­˜ä¸ºCSV
    val_df = pd.DataFrame({'guid': val_guids})
    save_path = os.path.join(config['output_dir'], 'val_guids.csv')
    val_df.to_csv(save_path, index=False)
    
    print(f"âœ… éªŒè¯é›†GUIDå·²ä¿å­˜: {save_path}")
    print(f"   æ ·æœ¬æ•°: {len(val_guids)}")
    print(f"   å‰5ä¸ªGUID: {val_guids[:5]}")
    
    # éªŒè¯åˆ†å¸ƒï¼ˆå¯é€‰ï¼‰
    print("\nâ–¶ éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:")
    labels = []
    for batch in val_loader:
        labels.extend(batch['label'].numpy().tolist())
    label_map = {0: 'positive', 1: 'neutral', 2: 'negative'}
    for i in range(3):
        count = labels.count(i)
        print(f"   {label_map[i]:12s}: {count:4d} æ ·æœ¬ ({count/len(labels)*100:5.2f}%)")
    
    return val_guids

def main():
    """Spyderä¸»å…¥å£ - ç›´æ¥è¿è¡Œæ­¤å‡½æ•°"""
    # 1. è®¾ç½®éšæœºç§å­
    set_seed(CONFIG['seed'])
    
    # 2. åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = Trainer(CONFIG)
    
    # ä¿å­˜éªŒè¯é›†ç”¨
    # save_validation_guids(CONFIG)
    # sys.exit(0)
    
    # 3. æ‰§è¡Œä»»åŠ¡
    if CONFIG['mode'] in ['train', 'train_test']:
        trainer.train()
    
    if CONFIG['mode'] in ['test', 'train_test']:
        trainer.test()
    
    print("\nğŸ‰ ä»»åŠ¡å®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨è¾“å‡ºç›®å½•:")
    print(f"   æ¨¡å‹: {os.path.join(CONFIG['output_dir'], 'best_model.pth')}")
    print(f"   é¢„æµ‹: {os.path.join(CONFIG['output_dir'], 'submission.csv')}")
    


if __name__ == "__main__":
    main()
    